{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5a99ff",
   "metadata": {},
   "source": [
    "# Autoencoder + Transformer Hybrid Model with SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f9f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\binwa\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "# Ensure TensorFlow is installed in your environment: pip install tensorflow numpy pandas scikit-learn matplotlib seaborn shap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d6bae",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ac148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for NSL-KDD dataset\n",
    "c_names = [\n",
    "    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\",\n",
    "    \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\",\n",
    "    \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\",\n",
    "    \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"labels\", \"difficulty_degree\"\n",
    "]\n",
    "\n",
    "# Load training and testing datasets\n",
    "train = pd.read_csv(\"data/KDDTrain+.txt\", names=c_names)\n",
    "test = pd.read_csv(\"data/KDDTest+.txt\", names=c_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a6556",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16be7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'difficulty_degree' column as it does not add value\n",
    "del train[\"difficulty_degree\"]\n",
    "del test[\"difficulty_degree\"]\n",
    "\n",
    "# Convert categorical features to numerical\n",
    "categorical_features = [\"protocol_type\", \"service\", \"flag\"]\n",
    "\n",
    "for col in categorical_features:\n",
    "    train[col] = train[col].astype(\"category\").cat.codes\n",
    "    test[col] = test[col].astype(\"category\").cat.codes\n",
    "\n",
    "# Map 'labels' column to binary classes (1 for 'normal', 0 for 'attack')\n",
    "train[\"labels\"] = train[\"labels\"].apply(lambda x: 1 if x == \"normal\" else 0)\n",
    "test[\"labels\"] = test[\"labels\"].apply(lambda x: 1 if x == \"normal\" else 0)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train.drop(\"labels\", axis=1)\n",
    "y_train = train[\"labels\"]\n",
    "X_test = test.drop(\"labels\", axis=1)\n",
    "y_test = test[\"labels\"]\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c098b",
   "metadata": {},
   "source": [
    "## Step 3: Define Autoencoder + Transformer Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b53011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder...\n",
      "Epoch 1/20\n",
      "1575/1575 [==============================] - 8s 5ms/step - loss: 0.7453 - val_loss: 0.7088\n",
      "Epoch 2/20\n",
      "1575/1575 [==============================] - 7s 5ms/step - loss: 0.7164 - val_loss: 0.6978\n",
      "Epoch 3/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7125 - val_loss: 0.6961\n",
      "Epoch 4/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7097 - val_loss: 0.6922\n",
      "Epoch 5/20\n",
      "1575/1575 [==============================] - 6s 4ms/step - loss: 0.7074 - val_loss: 0.6919\n",
      "Epoch 6/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7073 - val_loss: 0.6919\n",
      "Epoch 7/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7071 - val_loss: 0.6913\n",
      "Epoch 8/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7057 - val_loss: 0.6903\n",
      "Epoch 9/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7049 - val_loss: 0.6869\n",
      "Epoch 10/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7017 - val_loss: 0.6870\n",
      "Epoch 11/20\n",
      "1575/1575 [==============================] - 6s 4ms/step - loss: 0.7014 - val_loss: 0.6865\n",
      "Epoch 12/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7013 - val_loss: 0.6863\n",
      "Epoch 13/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7014 - val_loss: 0.6865\n",
      "Epoch 14/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7013 - val_loss: 0.6866\n",
      "Epoch 15/20\n",
      "1575/1575 [==============================] - 7s 5ms/step - loss: 0.7013 - val_loss: 0.6862\n",
      "Epoch 16/20\n",
      "1575/1575 [==============================] - 7s 4ms/step - loss: 0.7011 - val_loss: 0.6862\n",
      "Epoch 17/20\n",
      "1575/1575 [==============================] - 6s 4ms/step - loss: 0.7013 - val_loss: 0.6861\n",
      "Epoch 18/20\n",
      "1575/1575 [==============================] - 7s 5ms/step - loss: 0.7010 - val_loss: 0.6861\n",
      "Epoch 19/20\n",
      "1575/1575 [==============================] - 7s 5ms/step - loss: 0.7009 - val_loss: 0.6861\n",
      "Epoch 20/20\n",
      "1575/1575 [==============================] - 7s 5ms/step - loss: 0.7010 - val_loss: 0.6861\n",
      "3150/3150 [==============================] - 6s 2ms/step\n",
      "788/788 [==============================] - 1s 2ms/step\n",
      "705/705 [==============================] - 1s 2ms/step\n",
      "Training Transformer...\n",
      "Epoch 1/20\n",
      "1575/1575 [==============================] - 23s 14ms/step - loss: 0.0736 - accuracy: 0.9722 - val_loss: 0.0459 - val_accuracy: 0.9803\n",
      "Epoch 2/20\n",
      "1575/1575 [==============================] - 21s 13ms/step - loss: 0.0433 - accuracy: 0.9839 - val_loss: 0.0356 - val_accuracy: 0.9865\n",
      "Epoch 3/20\n",
      "1575/1575 [==============================] - 21s 13ms/step - loss: 0.0378 - accuracy: 0.9862 - val_loss: 0.0277 - val_accuracy: 0.9903\n",
      "Epoch 4/20\n",
      "1575/1575 [==============================] - 22s 14ms/step - loss: 0.0342 - accuracy: 0.9875 - val_loss: 0.0283 - val_accuracy: 0.9901\n",
      "Epoch 5/20\n",
      "1575/1575 [==============================] - 21s 13ms/step - loss: 0.0321 - accuracy: 0.9886 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
      "Epoch 6/20\n",
      "1575/1575 [==============================] - 22s 14ms/step - loss: 0.0303 - accuracy: 0.9889 - val_loss: 0.0256 - val_accuracy: 0.9901\n",
      "Epoch 7/20\n",
      "1572/1575 [============================>.] - ETA: 0s - loss: 0.0288 - accuracy: 0.9897"
     ]
    }
   ],
   "source": [
    "# Autoencoder Component\n",
    "def build_autoencoder(input_dim):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    encoded = layers.Dense(128, activation=\"relu\")(inputs)\n",
    "    encoded = layers.Dense(64, activation=\"relu\")(encoded)\n",
    "    encoded = layers.Dense(32, activation=\"relu\")(encoded)\n",
    "    decoded = layers.Dense(64, activation=\"relu\")(encoded)\n",
    "    decoded = layers.Dense(128, activation=\"relu\")(decoded)\n",
    "    outputs = layers.Dense(input_dim, activation=\"sigmoid\")(decoded)\n",
    "    autoencoder = keras.Model(inputs, outputs, name=\"autoencoder\")\n",
    "    encoder = keras.Model(inputs, encoded, name=\"encoder\")\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Transformer Component\n",
    "def build_transformer(input_shape):\n",
    "    inputs = keras.Input(shape=(input_shape,))\n",
    "    x = tf.expand_dims(inputs, axis=1)  # Add sequence dimension\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    for _ in range(2):\n",
    "        x1 = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
    "        x1 = layers.Add()([x, x1])\n",
    "        x1 = layers.LayerNormalization()(x1)\n",
    "        x2 = layers.Dense(32, activation=\"relu\")(x1)\n",
    "        x2 = layers.Dropout(0.3)(x2)\n",
    "        x = layers.Add()([x1, x2])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"transformer\")\n",
    "    return model\n",
    "\n",
    "# Combine Autoencoder and Transformer\n",
    "input_dim = x_train.shape[1]\n",
    "autoencoder, encoder = build_autoencoder(input_dim)\n",
    "transformer = build_transformer(32)  # Latent dimension from encoder\n",
    "\n",
    "# Compile Autoencoder\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "print(\"Training Autoencoder...\")\n",
    "autoencoder.fit(x_train, x_train, epochs=20, batch_size=64, validation_data=(x_val, x_val))\n",
    "\n",
    "# Encode Features\n",
    "encoded_x_train = encoder.predict(x_train)\n",
    "encoded_x_val = encoder.predict(x_val)\n",
    "encoded_x_test = encoder.predict(X_test)\n",
    "\n",
    "# Compile Transformer\n",
    "transformer.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(\"Training Transformer...\")\n",
    "history = transformer.fit(encoded_x_train, y_train, epochs=20, batch_size=64, validation_data=(encoded_x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6476a6",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eede46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = transformer.evaluate(encoded_x_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ac614",
   "metadata": {},
   "source": [
    "## Step 5: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cf011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Transformer Model Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Transformer Model Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "y_pred = (transformer.predict(encoded_x_test) > 0.5).astype(\"int32\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Attack\", \"Normal\"], yticklabels=[\"Attack\", \"Normal\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Attack\", \"Normal\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795087c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ecdffa",
   "metadata": {},
   "source": [
    "## Step 6: SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain predictions with SHAP\n",
    "explainer = shap.KernelExplainer(transformer.predict, encoded_x_test[:100])  # Use a subset for SHAP due to compute constraints\n",
    "shap_values = explainer.shap_values(encoded_x_test[:100])\n",
    "\n",
    "# Summary Plot\n",
    "shap.summary_plot(shap_values[0], encoded_x_test[:100], feature_names=train.columns[:-1])\n",
    "\n",
    "# Force Plot (First Instance)\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], encoded_x_test[:1], feature_names=train.columns[:-1])\n",
    "\n",
    "# Feature Importance Plot\n",
    "shap.summary_plot(shap_values, encoded_x_test[:100], plot_type=\"bar\", feature_names=train.columns[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
