{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03444b7",
   "metadata": {},
   "source": [
    "# GPT for Tabular Classification with SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f709856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 17:59:55.456759: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-16 17:59:55.457879: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-16 17:59:55.484004: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-16 17:59:55.484581: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-16 17:59:55.971320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "# Ensure TensorFlow and Transformers libraries are installed in your environment:\n",
    "# pip install tensorflow numpy pandas scikit-learn matplotlib seaborn transformers shap\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c580e",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cd2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for NSL-KDD dataset\n",
    "c_names = [\n",
    "    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\",\n",
    "    \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\",\n",
    "    \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\",\n",
    "    \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"labels\", \"difficulty_degree\"\n",
    "]\n",
    "\n",
    "# Load training and testing datasets\n",
    "train = pd.read_csv(\"data/KDDTrain+.txt\", names=c_names)\n",
    "test = pd.read_csv(\"data/KDDTest+.txt\", names=c_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd92d1",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921700aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'difficulty_degree' column as it does not add value\n",
    "# Remove 'difficulty_degree' column if it exists\n",
    "if \"difficulty_degree\" in train.columns:\n",
    "    del train[\"difficulty_degree\"]\n",
    "\n",
    "if \"difficulty_degree\" in test.columns:\n",
    "    del test[\"difficulty_degree\"]\n",
    "\n",
    "\n",
    "# Convert categorical features to numerical\n",
    "categorical_features = [\"protocol_type\", \"service\", \"flag\"]\n",
    "\n",
    "for col in categorical_features:\n",
    "    train[col] = train[col].astype(\"category\").cat.codes\n",
    "    test[col] = test[col].astype(\"category\").cat.codes\n",
    "\n",
    "# Map 'labels' column to binary classes (1 for 'normal', 0 for 'attack')\n",
    "train[\"labels\"] = train[\"labels\"].apply(lambda x: 1 if x == \"normal\" else 0)\n",
    "test[\"labels\"] = test[\"labels\"].apply(lambda x: 1 if x == \"normal\" else 0)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train.drop(\"labels\", axis=1)\n",
    "y_train = train[\"labels\"]\n",
    "X_test = test.drop(\"labels\", axis=1)\n",
    "y_test = test[\"labels\"]\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert each row into a string format for tokenization\n",
    "train_sequences = [\" \".join(map(str, row)) for row in X_train]\n",
    "test_sequences = [\" \".join(map(str, row)) for row in X_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569769b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 18:00:23.365962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-16 18:00:23.377478: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "train_encodings = tokenizer(train_sequences, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_sequences, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Ensure labels are NumPy arrays and have the correct dtype\n",
    "y_train = y_train.to_numpy().astype(\"float32\")\n",
    "y_test = y_test.to_numpy().astype(\"float32\")\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings), y_train\n",
    ")).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings), y_test\n",
    ")).batch(32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b9b4a",
   "metadata": {},
   "source": [
    "## Step 3: Define the GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389ca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GPT Model...\n",
      "Epoch 1/3\n",
      " 886/3937 [=====>........................] - ETA: 1:27:46 - loss: 0.1232 - accuracy: 0.9556"
     ]
    }
   ],
   "source": [
    "# Define the GPT Model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "# Compile the model with correct loss function\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training GPT Model...\")\n",
    "history = model.fit(train_dataset, epochs=3,    batch_size=64, validation_data=test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5371602e",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8a756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GPT Model...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 1588, in compute_loss  *\n        return super().compute_loss(*args, **kwargs)\n    File \"/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss  **\n        return self.compiled_loss(\n    File \"/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 263, in __call__\n        y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n    File \"/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 840, in match_dtype_and_rank\n        if (y_t.dtype.is_floating and y_p.dtype.is_floating) or (\n\n    AttributeError: 'NoneType' object has no attribute 'dtype'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the Model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining GPT Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileecrb7mo2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:1706\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1703\u001b[0m             y_pred \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1706\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_loss(y, y_pred, sample_weight, regularization_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables, tape\u001b[38;5;241m=\u001b[39mtape)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file4gmpxocq.py:38\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mhasattr\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(keras)\u001b[38;5;241m.\u001b[39mModel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompute_loss\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_return\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretval_\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fscope\u001b[38;5;241m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file4gmpxocq.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss.<locals>.if_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28msuper\u001b[39m), (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\u001b[38;5;241m.\u001b[39mcompute_loss, \u001b[38;5;28mtuple\u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(args)), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(kwargs)), fscope)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 1588, in compute_loss  *\n        return super().compute_loss(*args, **kwargs)\n    File \"/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss  **\n        return self.compiled_loss(\n    File \"/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 263, in __call__\n        y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n    File \"/home/wakili/anaconda3/envs/mlenv/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 840, in match_dtype_and_rank\n        if (y_t.dtype.is_floating and y_p.dtype.is_floating) or (\n\n    AttributeError: 'NoneType' object has no attribute 'dtype'\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "print(\"Training GPT Model...\")\n",
    "history = model.fit(train_dataset, epochs=3, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438d4aa",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ff32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "print(\"Evaluating GPT Model...\")\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfdc3d",
   "metadata": {},
   "source": [
    "## Step 6: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e40875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Model Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4280546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "y_pred = tf.argmax(model.predict(test_dataset).logits, axis=1).numpy()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Attack\", \"Normal\"], yticklabels=[\"Attack\", \"Normal\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Attack\", \"Normal\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2711e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1])\n",
    "y_pred_proba = tf.nn.softmax(model.predict(test_dataset).logits)[:, 1].numpy()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_binarized, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070ca79",
   "metadata": {},
   "source": [
    "## Step 7: SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis with Partial Data\n",
    "import shap\n",
    "\n",
    "# Select a subset of the data for SHAP analysis\n",
    "subset_size = 100  # Adjust the number of samples as needed\n",
    "X_test_subset = X_test[:subset_size]\n",
    "x_train_subset = x_train[:subset_size]\n",
    "\n",
    "# Initialize the SHAP explainer using the subset of training data\n",
    "explainer = shap.Explainer(model.predict, x_train_subset)\n",
    "\n",
    "# Generate SHAP values for the test subset\n",
    "shap_values = explainer(X_test_subset)\n",
    "\n",
    "# Define feature names explicitly as a list\n",
    "feature_names = list(train.columns[:-1])  # Convert to list for SHAP compatibility\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test_subset, feature_names=feature_names)\n",
    "plt.savefig(\"visualization/shap_summary_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# SHAP Decision Plot\n",
    "shap.decision_plot(\n",
    "    shap_values.base_values[0], \n",
    "    shap_values.values[0], \n",
    "    feature_names=feature_names\n",
    ")\n",
    "plt.savefig(\"visualization/shap_decision_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# SHAP Force Plot (Single Instance)\n",
    "shap.force_plot(\n",
    "    shap_values.base_values[0], \n",
    "    shap_values.values[0], \n",
    "    X_test_subset[0], \n",
    "    feature_names=feature_names, \n",
    "    matplotlib=True\n",
    ")\n",
    "shap.save_html(\n",
    "    \"visualization/shap_force_plot.html\", \n",
    "    shap.force_plot(\n",
    "        shap_values.base_values[0], \n",
    "        shap_values.values[0], \n",
    "        X_test_subset[0], \n",
    "        feature_names=feature_names\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
